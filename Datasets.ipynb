{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            UserId        date  hour                   Location  \\\n",
      "0   A1BG8QW55XHN6U  2020-07-28    19            Ontario, Canada   \n",
      "1   A1BG8QW55XHN6U  2020-07-28    19            Ontario, Canada   \n",
      "2   A1WMRR494NWEWV  2020-07-28    19       Texas, United States   \n",
      "3   A1WMRR494NWEWV  2020-07-28    19       Texas, United States   \n",
      "4   A22VW0P4VZHDE3  2020-07-28    19       Texas, United States   \n",
      "5   A39HTATAQ9V7YF  2020-07-28    18  California, United States   \n",
      "6   A39HTATAQ9V7YF  2020-07-28    18  California, United States   \n",
      "7   A3JM6GV9MNOF9X  2020-07-28    18  California, United States   \n",
      "8   A3JM6GV9MNOF9X  2020-07-28    18  California, United States   \n",
      "9   A3V3RE4132GKRO  2020-07-28    19       Texas, United States   \n",
      "10  A3V3RE4132GKRO  2020-07-28    19       Texas, United States   \n",
      "11   AKJHHD5VEH7VG  2020-07-28    19        Ohio, United States   \n",
      "12   AKJHHD5VEH7VG  2020-07-28    19        Ohio, United States   \n",
      "13           guest  2020-07-28    18  California, United States   \n",
      "\n",
      "    product_to_cart  timespentdaily  pages_visited  \n",
      "0                 0             195              3  \n",
      "1                 1              41              1  \n",
      "2                 0             272              3  \n",
      "3                 1              60              1  \n",
      "4                 0             204              4  \n",
      "5                 0              54              4  \n",
      "6                 1              66              1  \n",
      "7                 0             482              4  \n",
      "8                 1             148              3  \n",
      "9                 0             349              4  \n",
      "10                1             121              2  \n",
      "11                0             106              3  \n",
      "12                1             102              3  \n",
      "13                0             198              3  \n",
      "    timespent_perday  no_pages_visited                   location  \\\n",
      "0                195                 3            Ontario, Canada   \n",
      "1                 41                 1            Ontario, Canada   \n",
      "2                272                 3       Texas, United States   \n",
      "3                 60                 1       Texas, United States   \n",
      "4                204                 4       Texas, United States   \n",
      "5                 54                 4  California, United States   \n",
      "6                 66                 1  California, United States   \n",
      "7                482                 4  California, United States   \n",
      "8                148                 3  California, United States   \n",
      "9                349                 4       Texas, United States   \n",
      "10               121                 2       Texas, United States   \n",
      "11               106                 3        Ohio, United States   \n",
      "12               102                 3        Ohio, United States   \n",
      "13               198                 3  California, United States   \n",
      "\n",
      "    hour_session_starts was_there_purchase  \n",
      "0                    19                 No  \n",
      "1                    19                Yes  \n",
      "2                    19                 No  \n",
      "3                    19                Yes  \n",
      "4                    19                 No  \n",
      "5                    18                 No  \n",
      "6                    18                Yes  \n",
      "7                    18                 No  \n",
      "8                    18                Yes  \n",
      "9                    19                 No  \n",
      "10                   19                Yes  \n",
      "11                   19                 No  \n",
      "12                   19                Yes  \n",
      "13                   18                 No  \n",
      "Data Table saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Balli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, field_validator, ValidationError\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = os.path.abspath('C:/Users/Balli/Pictures')\n",
    "\n",
    "data_weblog = pd.read_csv(path +'/store_web_log.csv')\n",
    "\n",
    "ip_info= pd.read_csv(path +'/ip_info.csv')\n",
    "\n",
    "#perform a join querry of our two dataset that are related \n",
    "merged_data=pd.merge(data_weblog,ip_info, on='IP',how='left')\n",
    "\n",
    "# converting to datetime to be able to get the date and hour of  daily transaction\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'],unit='s')\n",
    "\n",
    "# filtering the date out of the datetime\n",
    "merged_data['date'] = merged_data['timestamp'].dt.date\n",
    "\n",
    "#filtering the hour of user session out of datetime\n",
    "merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "\n",
    "\n",
    "grouped_Data=merged_data.groupby(['UserId','date','hour','Location','product_to_cart']).agg(\n",
    "                                timespentdaily=('Duration','sum'),pages_visited=('Address','count')).reset_index()\n",
    "print(grouped_Data)\n",
    "\n",
    "FinalData=grouped_Data.drop_duplicates(subset=['UserId'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# defining the customer web access information table\n",
    "\n",
    "class customer_web_info_model(BaseModel):\n",
    "    timespent_perday: int\n",
    "    no_pages_visited: int\n",
    "    location: str\n",
    "    hour_session_starts: int\n",
    "    was_there_purchase: int\n",
    "    \n",
    "    \n",
    "    # creating validation for column number of pages visited\n",
    "    @field_validator('no_pages_visited')\n",
    "    def check_pages_visited(cls, value):\n",
    "        if value <= 0:\n",
    "            raise ValidationError('Pages visited field can not be less than 0')\n",
    "        else:\n",
    "            return value\n",
    "        \n",
    "   # creating validation for column was there a purchase to convert integer to a string answer of Yes or No \n",
    "    @field_validator('was_there_purchase')\n",
    "    def purchase(cls, value):\n",
    "        if value == 1:\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "\n",
    "\n",
    "# defining a variable list to store our data from the model customer_web_info       \n",
    "data_list = []\n",
    "# Iterating over the group_Data to load our model with data\n",
    "for index, row in grouped_Data.iterrows():\n",
    "    try:    \n",
    "        dataTable1 = customer_web_info_model(timespent_perday=row['timespentdaily'] ,\n",
    "                                     no_pages_visited=row['pages_visited'],\n",
    "                                     location=row['Location'],hour_session_starts=row['hour'],\n",
    "                                    was_there_purchase=row['product_to_cart']\n",
    "                                     )\n",
    "        \n",
    "        data_list.append(dataTable1.model_dump())\n",
    "    except Exception as error:\n",
    "        print(f'error:{error}')\n",
    "\n",
    "# converting data list to pandas dataframe\n",
    "if data_list:\n",
    "    df_data_list=pd.DataFrame(data_list)\n",
    "    print(df_data_list)\n",
    "    \n",
    "    #saving our file/dataset as csv\n",
    "    df_data_list.to_csv('Data Table 1.csv', index=False)\n",
    "    print('Data Table saved successfully')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  houroftransactn   productname  price\n",
      "0  2020-07-28               18     Red Pants  10.99\n",
      "1  2020-07-28               19     Red Pants  10.99\n",
      "2  2020-07-28               20  Flower Dress  25.99\n",
      "3  2020-07-28               20   Orange Belt  13.99\n",
      "4  2020-07-28               20   Pink Shorts   9.99\n",
      "5  2020-07-28               21     Red Pants  10.99\n",
      "6  2020-07-28               21  Flower Dress  25.99\n",
      "7  2020-07-28               21   Orange Belt  13.99\n",
      "8  2020-07-28               21  Yellow Skirt  17.99\n",
      "9  2020-08-02                8     Red Pants  10.99\n",
      "     productname   price       date  hour_of_transaction\n",
      "0      Red Pants  $10.99 2020-07-28                   18\n",
      "1      Red Pants  $10.99 2020-07-28                   19\n",
      "2   Flower Dress  $25.99 2020-07-28                   20\n",
      "3    Orange Belt  $13.99 2020-07-28                   20\n",
      "4    Pink Shorts   $9.99 2020-07-28                   20\n",
      "5      Red Pants  $10.99 2020-07-28                   21\n",
      "6   Flower Dress  $25.99 2020-07-28                   21\n",
      "7    Orange Belt  $13.99 2020-07-28                   21\n",
      "8   Yellow Skirt  $17.99 2020-07-28                   21\n",
      "9      Red Pants  $10.99 2020-08-02                    8\n",
      "10   Pink Shorts   $9.99 2020-08-02                   11\n",
      "11    Blue Pants  $11.99 2020-08-02                   11\n",
      "12   Green Pants  $12.99 2020-08-02                   11\n",
      "13  Yellow Skirt  $17.99 2020-08-02                   11\n",
      "14     Red Pants  $10.99 2020-08-02                   11\n",
      "15    Blue Pants  $11.99 2020-08-02                   11\n",
      "16   Green Pants  $12.99 2020-08-02                   11\n",
      "17    Blue Pants  $11.99 2020-08-02                   11\n",
      "18   Green Pants  $12.99 2020-08-02                   11\n",
      "19     Red Pants  $10.99 2020-08-02                   12\n",
      "20  Yellow Skirt  $17.99 2020-08-02                   12\n",
      "21     Red Pants  $10.99 2020-08-02                   12\n",
      "22   Pink Shorts   $9.99 2020-08-02                   12\n",
      "23  Yellow Skirt  $17.99 2020-08-02                   12\n",
      "24    Blue Pants  $11.99 2020-08-02                   12\n",
      "25   Green Pants  $12.99 2020-08-02                   12\n",
      "26   Pink Shorts   $9.99 2020-08-02                   12\n",
      "27     Red Pants  $10.99 2020-08-02                   14\n",
      "28   Pink Shorts   $9.99 2020-08-02                   16\n",
      "29  Yellow Skirt  $17.99 2020-08-02                   17\n",
      "30     Red Pants  $10.99 2020-08-07                    8\n",
      "31  Flower Dress  $25.99 2020-08-07                    8\n",
      "32   Orange Belt  $13.99 2020-08-07                    8\n",
      "33    Blue Pants  $11.99 2020-08-07                   13\n",
      "34   Green Pants  $12.99 2020-08-07                   13\n",
      "35     Red Pants  $10.99 2020-08-07                   14\n",
      "36    Blue Pants  $11.99 2020-08-07                   16\n",
      "37   Green Pants  $12.99 2020-08-07                   16\n",
      "38     Red Pants  $10.99 2020-08-07                   18\n",
      "39  Flower Dress  $25.99 2020-08-07                   20\n",
      "40   Orange Belt  $13.99 2020-08-07                   20\n",
      "Data Table saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Balli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `float` but got `str` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, field_validator, ValidationError\n",
    "from datetime import  datetime\n",
    "import os\n",
    "\n",
    "path = os.path.abspath('C:/Users/Balli/Pictures')\n",
    "\n",
    "productData= pd.read_csv(path +'/store_product_table.csv')\n",
    "\n",
    "transactionData= pd.read_csv(path +'/store_transaction_records.csv')\n",
    "\n",
    "#performing a join querry of thw two dataset related by transaction\n",
    "merged_data1 = pd.merge(transactionData,productData, on='ProductID',how='left')\n",
    "\n",
    "# converting to datetime to be able to get the date and hour of  daily transaction\n",
    "merged_data1['transactn_time'] = pd.to_datetime(merged_data1['time_transactn'],unit='s')\n",
    "\n",
    "# filtering the date out of the datetime\n",
    "merged_data1['date'] = merged_data1['transactn_time'].dt.date\n",
    "\n",
    "#filtering the hour of transaction out of datetime\n",
    "merged_data1['hour'] = merged_data1['transactn_time'].dt.hour\n",
    "\n",
    "#creating a dictionary of our new formated dataset\n",
    "DictData={'date':merged_data1['date'],'houroftransactn':merged_data1['hour'],\n",
    "            'productname':merged_data1['Productdescriptn'],'price':merged_data1['price']}\n",
    "\n",
    "#converting to pandas dataframe\n",
    "finaldata1=pd.DataFrame(DictData)\n",
    "print(finaldata1.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Defining the transacion table or model\n",
    "\n",
    "class transaction_model(BaseModel):\n",
    "    productname:str\n",
    "    price: float\n",
    "    date: datetime\n",
    "    hour_of_transaction: int\n",
    "\n",
    "    @field_validator('price')\n",
    "    def check_price(cls, value):\n",
    "        if value:\n",
    "            return f'${value}'\n",
    "        elif value <= 0:\n",
    "            raise ValidationError\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    \n",
    "datalist_2 = []\n",
    "# iterating over finaldata1 to load our model\n",
    "for index, row in finaldata1.iterrows():\n",
    "    dataTable2 = transaction_model(productname=row['productname'],date= row['date'], hour_of_transaction=row['houroftransactn'], \n",
    "                                   price=row['price'])\n",
    "    \n",
    "    datalist_2.append(dataTable2.model_dump())\n",
    "# converting datalist_2 to pandas dataframe\n",
    "if datalist_2:\n",
    "        datalist_2display=pd.DataFrame(datalist_2)\n",
    "        print(datalist_2display)\n",
    "        # saving our data table as csv\n",
    "        datalist_2display.to_csv('Data Table 2.csv', index=False)\n",
    "        print('Data Table saved successfully')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                last_web_pagevisited  no_of_time_pagevisited\n",
      "0  www.store.com/checkout_trans#1543                       1\n",
      "1  www.store.com/checkout_trans#1547                       1\n",
      "2  www.store.com/checkout_trans#1548                       1\n",
      "3       www.store.com/item#558925278                       1\n",
      "4                www.store.com/main#                       1\n",
      "5    www.store.com/ty#A39HTATAQ9V7YF                       1\n",
      "6     www.store.com/ty#AKJHHD5VEH7VG                       1\n",
      "Data Table saved successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, field_validator, ValidationError\n",
    "from datetime import  datetime\n",
    "import os\n",
    "#defining the path to our save dataset file\n",
    "path = os.path.abspath('C:/Users/Balli/Pictures')\n",
    "\n",
    "data_weblog = pd.read_csv(path +'/store_web_log.csv')\n",
    "\n",
    "\n",
    "# Filter rows where Link is null\n",
    "last_page_visits = data_weblog[data_weblog['Link'].isnull()]\n",
    "#print(last_page_visits)\n",
    "\n",
    "# Group by the Address column and count occurrences\n",
    "last_page_counts = last_page_visits.groupby('Address').size().reset_index(name='count')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# defining last  web page visited  count model\n",
    "\n",
    "\n",
    "class last_page_visited(BaseModel):\n",
    "    last_web_pagevisited: str\n",
    "    no_of_time_pagevisited: int\n",
    "\n",
    "#set a list variable to store our table data\n",
    "datatable3 = []\n",
    "\n",
    "#iterate over the last_page_count table and load into our model\n",
    "for index, row in last_page_counts.iterrows():\n",
    "    Data3=last_page_visited(last_web_pagevisited=row['Address'],no_of_time_pagevisited=row['count'])\n",
    "    datatable3.append(Data3.model_dump())\n",
    "#convert our data table 3 into pandas dataframe\n",
    "if datatable3:\n",
    "    df_datatable3=pd.DataFrame(datatable3)\n",
    "    print(df_datatable3)\n",
    "    # saving our file/data  as csv\n",
    "    df_datatable3.to_csv('Data Table 3.csv', index=False)\n",
    "    print('Data Table saved successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name                                      phone  age       income  \\\n",
      "0       Thom  <a href=\"Tel:+1604907178\">+1604907178</a>   67   $754,298.0   \n",
      "1       Thom  <a href=\"Tel:+1908876453\">+1908876453</a>   23     $9,000.0   \n",
      "2      Colin  <a href=\"Tel:+1604675894\">+1604675894</a>   54    $62,840.0   \n",
      "3      Colin  <a href=\"Tel:+1604675894\">+1604675894</a>   54    $62,840.0   \n",
      "4       Phil  <a href=\"Tel:+1509524718\">+1509524718</a>   12    $78,000.0   \n",
      "5         Ed  <a href=\"Tel:+1509826727\">+1509826727</a>   35   $445,566.0   \n",
      "6     Johnny  <a href=\"Tel:+1604918675\">+1604918675</a>   87   $212,121.0   \n",
      "7      Sarah  <a href=\"Tel:+1402289101\">+1402289101</a>   65    $36,000.0   \n",
      "8       Yoda  <a href=\"Tel:+1642098256\">+1642098256</a>   62    $49,382.0   \n",
      "9     Alexis  <a href=\"Tel:+1604897178\">+1604897178</a>   44    $12,344.0   \n",
      "10     Hanna  <a href=\"Tel:+1509786098\">+1509786098</a>   32     $1,010.0   \n",
      "11    Rachel  <a href=\"Tel:+1709878923\">+1709878923</a>   28   $100,000.0   \n",
      "12      Tina  <a href=\"Tel:+1509200775\">+1509200775</a>   19    $77,000.0   \n",
      "13      Paul  <a href=\"Tel:+1509817293\">+1509817293</a>   50  $136,938.92   \n",
      "14   Georgia  <a href=\"Tel:+1000090082\">+1000090082</a>   44  $136,938.92   \n",
      "15     Riana  <a href=\"Tel:+1377209445\">+1377209445</a>   44  $136,938.92   \n",
      "16       Pop  <a href=\"Tel:+1374429327\">+1374429327</a>   44  $136,938.92   \n",
      "17     Hanna  <a href=\"Tel:+1509786098\">+1509786098</a>   32     $1,010.0   \n",
      "18   Preston  <a href=\"Tel:+1709878923\">+1709878923</a>   54  $136,938.92   \n",
      "19    MayMay  <a href=\"Tel:+1070989875\">+1070989875</a>   44    $62,840.0   \n",
      "20    Brenda  <a href=\"Tel:+1709878954\">+1709878954</a>   38    $43,124.0   \n",
      "21    George  <a href=\"Tel:+1709878929\">+1709878929</a>   43    $62,840.0   \n",
      "22     Poppy  <a href=\"Tel:+1710000000\">+1710000000</a>   41   $100,100.0   \n",
      "23   Herbert  <a href=\"Tel:+1444878923\">+1444878923</a>   76    $23,876.0   \n",
      "24    Walker  <a href=\"Tel:+1234567890\">+1234567890</a>   44    $87,234.0   \n",
      "25   Barbara  <a href=\"Tel:+1454545454\">+1454545454</a>   60   $666,666.0   \n",
      "26     Laura  <a href=\"Tel:+1700707700\">+1700707700</a>   12    $15,678.0   \n",
      "27       Jeb  <a href=\"Tel:+1678975123\">+1678975123</a>   34   $223,000.0   \n",
      "28  Prescott  <a href=\"Tel:+1100000000\">+1100000000</a>   52    $99,765.0   \n",
      "\n",
      "           education  Gender  consent  \n",
      "0        High School    Male        0  \n",
      "1            College    Male        1  \n",
      "2            college    Male        1  \n",
      "3    graduate school    Male        0  \n",
      "4        High School    Male        1  \n",
      "5            college    Male        1  \n",
      "6            college    Male        0  \n",
      "7    graduate school  Female        0  \n",
      "8            college  Female        1  \n",
      "9        High School  Female        0  \n",
      "10       High School  Female        1  \n",
      "11       High School  Female        1  \n",
      "12           College  Female        1  \n",
      "13           College    Male        1  \n",
      "14           College  Female        1  \n",
      "15           college  Female        1  \n",
      "16           College    Male        1  \n",
      "17       High School  Female        0  \n",
      "18   graduate school    Male        1  \n",
      "19   graduate school  Female        0  \n",
      "20  graduate  school  Female        0  \n",
      "21           college    Male        0  \n",
      "22       High School    Male        1  \n",
      "23       High School    Male        1  \n",
      "24       High School    Male        1  \n",
      "25           college  Female        1  \n",
      "26       High School  Female        0  \n",
      "27       High School    Male        1  \n",
      "28       High School    Male        1  \n",
      "dataset saved in html file successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Balli\\AppData\\Local\\Temp\\ipykernel_6276\\3237296706.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  customerdataset['age'].fillna(median_age,inplace=True)\n",
      "C:\\Users\\Balli\\AppData\\Local\\Temp\\ipykernel_6276\\3237296706.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  customerdataset['education'].fillna(value='College',inplace=True)\n",
      "C:\\Users\\Balli\\AppData\\Local\\Temp\\ipykernel_6276\\3237296706.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  customerdataset['income'].fillna(mean_income,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, field_validator, ValidationError\n",
    "import phonenumbers\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "path = os.path.abspath('C:/Users/Balli/Pictures')\n",
    "\n",
    "customerdataset= pd.read_csv(path +'/customerdata.csv')\n",
    "#print(customerdataset)\n",
    "\n",
    "# The Gender column is empty, and since there is no module in python currently to decipher\n",
    "# male or female from the names. it became sacrosanct to use Excel and manually fill this column since\n",
    "# we can decipher male names from female names by bare looking at it.\n",
    "\n",
    "\n",
    "#customerdataset.info()\n",
    "#customerdataset.duplicated()\n",
    "#customerdataset.loc[customerdataset.duplicated(),:]\n",
    "\n",
    "\n",
    "# now let us deal wit outliers\n",
    "# from the dataset it can be noticed that the age column have an outlier which is 500, because no human in the generation\n",
    "# can live to 500 years\n",
    "\n",
    "# because this dataset is no many and we can easily spot outliers so we may not need to use IQR method but directly\n",
    "# replace the outlier with the mean or median or mode as the case may be, but for this dataset, i may have to use mean\n",
    "mean_age=customerdataset['age'].mean()\n",
    "mode_age=customerdataset['age'].mode()[0]\n",
    "median_age=customerdataset['age'].median()\n",
    "\n",
    "# Now i am going t replace the outlier with the mean_age\n",
    "\n",
    "customerdataset.loc[(customerdataset['age'] >= 500),'age']=mean_age\n",
    "\n",
    "# replacing the null value in the age column with the median age\n",
    "customerdataset['age'].fillna(median_age,inplace=True)\n",
    "\n",
    "# Age does not have decimal that is there is no half age. so we need to convert the age dtype to proper int\n",
    "customerdataset['age'] = customerdataset['age'].astype(int)\n",
    "\n",
    "#  Now cleaning the Education column and replacing the null with the most frequent education category\n",
    "\n",
    "customerdataset['education'].value_counts(dropna=False)\n",
    "customerdataset['education'].fillna(value='College',inplace=True)\n",
    "\n",
    "# stripping off excess whitespaces in the Education column\n",
    "customerdataset['education']=customerdataset['education'].str.strip()\n",
    "\n",
    "# Now cleaning the income column to replace the null with the mean income and adding the $ sign\n",
    "\n",
    "mean_income=round(customerdataset['income'].mean(),2)\n",
    "customerdataset['income'].fillna(mean_income,inplace=True)\n",
    "\n",
    "\n",
    "def add_currencySign(income):\n",
    "    if income:\n",
    "        return '${:,}'.format(income)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "customerdataset['income']=customerdataset['income'].apply(add_currencySign)\n",
    "\n",
    "\n",
    "# Now cleaning the Consent column to replace any string to integer and convert datatype to int\n",
    "customerdataset.loc[(customerdataset['consent'] == 'i'),'consent'] = 1\n",
    "customerdataset['consent']=customerdataset['consent'].astype(int)\n",
    "\n",
    "# Cleaning the pone number column and also creating a link for one-time click dial\n",
    "\n",
    "# Defining a function to clean and replace null phone column with a random generated number as phone\n",
    "\n",
    "def formated_phone(phone):\n",
    "        if pd.isnull(phone):\n",
    "          return  int(random.randint(10**9,10**10))\n",
    "        \n",
    "        else:\n",
    "            return int(phone)\n",
    " # Appling the function           \n",
    "customerdataset['phone']=customerdataset['phone'].apply(formated_phone)\n",
    "\n",
    "# Filling any phone number that is less 10 digit\n",
    "customerdataset['phone']=customerdataset['phone'].astype(str).str.zfill(10)\n",
    "\n",
    "# Trimming any phone number that is greater than 10 digits\n",
    "customerdataset['phone'] = customerdataset['phone'].astype(str).str.slice(0,9)\n",
    "\n",
    "# Now converting the phone number column to proper phone number standard using the python phonenumber module\n",
    "\n",
    "def format_phone_number(phone):\n",
    "    try:\n",
    "        # Parse the phone number\n",
    "        \n",
    "        parsed_number = phonenumbers.parse(phone, \"CA\")\n",
    "        \n",
    "        # Format the phone number in E.164 format\n",
    "        formatted_number = phonenumbers.format_number(parsed_number, phonenumbers.PhoneNumberFormat.E164)\n",
    "        \n",
    "        return formatted_number\n",
    "    \n",
    "    except phonenumbers.NumberParseException:\n",
    "        return phone  # Return the original phone number if parsing fails\n",
    "    \n",
    "# Now applying the function format_phone_number to the phone column of our dataset\n",
    "customerdataset['phone']=customerdataset['phone'].apply(format_phone_number)\n",
    "\n",
    "# Final step is to create a link on the phone column so that this can aid auto-dail by creating a function\n",
    "\n",
    "def clickable_phone(phone):\n",
    "    if phone:\n",
    "        return f'<a href=\"Tel:{phone}\">{phone}</a>'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "customerdataset['phone']=customerdataset['phone'].apply(clickable_phone)\n",
    "\n",
    "print(customerdataset)\n",
    "\n",
    "# To enable the customer rep to dial customer phone numbers automatically, we need to save te dataset in an html format\n",
    "# this will enable the dataset to e opened through a browser and then giving the link to auto dial a number\n",
    "\n",
    "customerdataset.to_html('customerdataset.html',escape=False,index=False)\n",
    "print('dataset saved in html file successfully')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
